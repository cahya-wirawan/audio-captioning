{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import audiocap\n",
    "import transformers\n",
    "\n",
    "from audiocap.train_whisper_supervised import get_whisper_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lovely_numpy\n",
    "import lovely_tensors\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def lovely(x):\n",
    "    if isinstance(x, np.ndarray):\n",
    "        return lovely_numpy.lovely(x)\n",
    "    if isinstance(x, torch.Tensor):\n",
    "        return lovely_tensors.lovely(x)\n",
    "    if isinstance(x, str):\n",
    "        return x\n",
    "    try:\n",
    "        return \"len: \" + str(len(x))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "def inspect(ds, samples=1):\n",
    "    for i, sample in enumerate(ds):\n",
    "        if i >= samples:\n",
    "            break\n",
    "\n",
    "        for key, value in sample.items():\n",
    "            print(\"  \", key, type(value), end=\"\")\n",
    "            if lovely(value) is not None:\n",
    "                print(\":\", lovely(value), end=\"\")\n",
    "            print()\n",
    "        if \"audio\" in sample:\n",
    "            print(\"  \", \"audio->array\", lovely(sample[\"audio\"][\"array\"]))\n",
    "        \n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "architecture_name = \"openai/whisper-tiny\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = transformers.WhisperConfig.from_pretrained(architecture_name)\n",
    "model = transformers.WhisperConfig.from_pretrained(architecture_name)\n",
    "tokenizer = transformers.WhisperTokenizer.from_pretrained(architecture_name, language=\"en\", task=\"transcribe\")\n",
    "feature_extractor = transformers.WhisperFeatureExtractor.from_pretrained(architecture_name)\n",
    "assert isinstance(config, transformers.WhisperConfig)\n",
    "model = get_whisper_model(architecture_name, config, False, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69b22fd6672048e596ca8e4447de0edb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/128434 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset audiofolder (/var/tmp/xkadlci2/.cache/huggingface/datasets/audiofolder/default-20a7d1f57c344d53/0.0.0/6cbdd16f8688354c63b4e2a36e1585d05de285023ee6443ffd71c4182055c0fc)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9abe6db86a76463c9dc67e77bece21cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58c28db4db7e4e17b5140fa018456f2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/2727 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset audiofolder (/var/tmp/xkadlci2/.cache/huggingface/datasets/audiofolder/default-cb14e1e33eee7836/0.0.0/6cbdd16f8688354c63b4e2a36e1585d05de285023ee6443ffd71c4182055c0fc)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00786a59ae044e309ab7b5885ee3dd78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b779cbca91e94f89a5cff8fe7a7f55ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/2844 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset audiofolder (/var/tmp/xkadlci2/.cache/huggingface/datasets/audiofolder/default-858ce7cd649041a3/0.0.0/6cbdd16f8688354c63b4e2a36e1585d05de285023ee6443ffd71c4182055c0fc)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1db8fae6182f4519836753b73e7ccdf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44f21b6f87ec4228b1748ff0c09f9a21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/45395 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset audiofolder (/var/tmp/xkadlci2/.cache/huggingface/datasets/audiofolder/default-5e5b67b602b44fdb/0.0.0/6cbdd16f8688354c63b4e2a36e1585d05de285023ee6443ffd71c4182055c0fc)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a015bd9233ce4c5ea5b5ca023c165c8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "819685ce916b4b918da5f00b5952d587",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/449 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset audiofolder (/var/tmp/xkadlci2/.cache/huggingface/datasets/audiofolder/default-b53c2008566841d9/0.0.0/6cbdd16f8688354c63b4e2a36e1585d05de285023ee6443ffd71c4182055c0fc)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdf8105160f843f2bc2163e7cfcffdb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb8ee738641a419faebb77568fea2aad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/887 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset audiofolder (/var/tmp/xkadlci2/.cache/huggingface/datasets/audiofolder/default-1af03cfbd7a86dba/0.0.0/6cbdd16f8688354c63b4e2a36e1585d05de285023ee6443ffd71c4182055c0fc)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c89c3f6763ab4cc1b57b1cf07351bbb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pathlib\n",
    "import datasets\n",
    "\n",
    "audioset_dir = pathlib.Path(\"../data/audioset_small_dummy/\")\n",
    "audiocaps_dir = pathlib.Path(\"../data/audiocaps_dummy/\")\n",
    "\n",
    "ds_audioset = audiocap.data.load_audioset_small(\n",
    "    audioset_dir / \"audiofolder\",\n",
    "    audioset_dir / \"annotations/ontology.json\",\n",
    ")\n",
    "\n",
    "ds_audiocaps = audiocap.data.load_audiocaps(audiocaps_dir / \"audiofolder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def gen_1():\n",
    "    yield {\"audio\": {\"array\": np.zeros(150)}, \"b\": np.ones(200), \"input_features\": np.ones((80, 3000))}\n",
    "    yield {\"audio\": {\"array\": np.zeros(180)}, \"b\": np.ones(300), \"input_features\": np.ones((80, 3000))}\n",
    "\n",
    "def gen_2():\n",
    "    yield {\"audio\": {\"array\": np.zeros(100)}, \"b\": np.ones(200), \"input_features\": np.ones((80, 3000))}\n",
    "    yield {\"audio\": {\"array\": np.zeros(100)}, \"b\": np.ones(300), \"input_features\": np.ones((80, 3000))}\n",
    "\n",
    "foo_1 = datasets.IterableDataset.from_generator(gen_1)\n",
    "foo_2 = datasets.IterableDataset.from_generator(gen_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "interleaved = audiocap.data.interleave_datasets([foo_1, foo_2], stop_on_first_end=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   audio <class 'dict'>: len: 1\n",
      "   b <class 'numpy.ndarray'>: array[200] x∈[1.000, 1.000] μ=1.000 σ=0.\n",
      "   input_features <class 'numpy.ndarray'>: array[80, 3000] n=240000 x∈[1.000, 1.000] μ=1.000 σ=0.\n",
      "   audio->array array[100] \u001b[38;2;127;127;127mall_zeros\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inspect(interleaved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "interleaved = {\n",
    "    \"train\": audiocap.data.interleave_datasets([ds_audioset[\"train\"], ds_audiocaps[\"train\"]], stop_on_first_end=False),\n",
    "    \"val\": audiocap.data.interleave_datasets([ds_audioset[\"val\"], ds_audiocaps[\"val\"]], stop_on_first_end=False),\n",
    "    \"test\": audiocap.data.interleave_datasets([ds_audioset[\"test\"], ds_audiocaps[\"test\"]], stop_on_first_end=False),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing = audiocap.preprocess.Preprocess(tokenizer, feature_extractor)\n",
    "\n",
    "for split in [\"train\", \"val\", \"test\"]:\n",
    "    interleaved[split] = interleaved[split].map(\n",
    "        preprocessing,\n",
    "        batched=True,\n",
    "        batch_size=16,\n",
    "        remove_columns=[\"audio\", \"prefix\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   caption <class 'str'>: arrow, onomatopoeia, whoosh - swoosh - swish\n",
      "   caption_colname <class 'str'>: labels\n",
      "   source_ds <class 'str'>: audioset\n",
      "   task <class 'str'>: keywords\n",
      "   path <class 'NoneType'>\n",
      "   audio_array <class 'numpy.ndarray'>: array[160497] x∈[-1.003, 1.014] μ=0.000 σ=0.113\n",
      "   sampling_rate <class 'int'>\n",
      "   forced_ac_decoder_ids <class 'list'>: len: 7\n",
      "   filename <class 'NoneType'>\n",
      "   input_features <class 'numpy.ndarray'>: array[80, 3000] f32 n=240000 x∈[-0.549, 1.451] μ=-0.404 σ=0.356\n",
      "   labels <class 'list'>: len: 29\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inspect(interleaved[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "audiocap",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
