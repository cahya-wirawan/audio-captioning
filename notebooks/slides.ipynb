{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\flori\\miniconda3\\envs\\malach23\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import librosa as liro\n",
    "import librosa.display as lirod\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "import IPython.display as ipd\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torchaudio\n",
    "from scipy.fft import rfft, rfftfreq\n",
    "import numpy as np\n",
    "\n",
    "wd = os.getcwd()\n",
    "os.chdir(os.path.join(wd, \"..\"))\n",
    "\n",
    "from models.mel import AugmentMelSTFT\n",
    "from datasets import audiodataset\n",
    "\n",
    "os.chdir(wd)\n",
    "\n",
    "example_data_path = os.path.join(\"..\", \"datasets\", \"example_data\", \"audio\")\n",
    "example_file = os.path.join(example_data_path, \"mixkit-dog-barking-twice-1.wav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Today's Menu\n",
    "\n",
    "- **Introduction to Pipeline**\n",
    "    * Tips & Tricks for building an ML pipeline\n",
    "    * Audio Preprocessing\n",
    "    * Example Pipeline on GitHub\n",
    "- **Your next task**\n",
    "- **Questions**\n",
    "- **Your reports**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction to Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Pipeline - Expectation\n",
    "\n",
    "\n",
    "<center>\n",
    "<img src=\"figs/expectation.jpg\" alt=\"drawing\" width=\"600\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Pipeline - Reality\n",
    "\n",
    "<center>\n",
    "<img src=\"figs/reality.jpg\" alt=\"drawing\" width=\"600\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Tips & Tricks for building an ML pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Best Practices\n",
    "\n",
    "* **document everything** (you in two weeks and your collegues should understand what you have done)\n",
    "* be careful and maybe **read things twice** (bugs in your pipeline may cost you hours later on)\n",
    "* **log everything**\n",
    "* use **version control** - commit early, commit often"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Documentation\n",
    "\n",
    "* keep a continuously updated **work log** - your colleagues should be able to quickly read up what you have done\n",
    "* a well-written work log will save you a lot of time when writing your **technical report**\n",
    "* put your worklog in version control (at least a google doc for collaborating with your colleagues)\n",
    "* collect questions to us in your work log\n",
    "* add enough comments to your code, **explaining \"the why\" of it**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Version Control\n",
    "\n",
    "* use **git**\n",
    "* get familiar with it\n",
    "* use one repo for code, one for work log\n",
    "* when collaborating, use your own **branches**\n",
    "* **merge frequently**\n",
    "* keep your master branch stable and runnable\n",
    "* check new features added in a branch together in the group before merging them to the master branch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Testing\n",
    "\n",
    "run regular tests, in particular:\n",
    "* **acquaintance tests**: get to know new library you don't know yet, allows to quickly learn new API, use e.g. a Jupyter Notebook\n",
    "* **unit tests**: simple tests for your own functions (e.g. data augmentation functions)\n",
    "* **integration tests**: more complex to test the interactions between your functions\n",
    "* **validity tests**: manual inspection (e.g. by looking at computed spectrograms) to check if your code works as expected\n",
    "\n",
    "**Try to verify each new code part in your group before you run full experiments with it.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Reproducibility\n",
    "\n",
    "* **Code**: be aware of dependencies, use virtual environments (conda) and include frozen dependencies in version control\n",
    "* **Training**: use a mother seed sequence to seed all pseudo random number generators (torch, numpy, python random, ...), log seed, log commit-hash\n",
    "* **Data**: make preprocessing of data and labels repeatable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Leakage\n",
    "\n",
    "* **use the provided split for your task**!\n",
    "* make sure **NO** aspects of your test data distribution are leaking into your training or validation set\n",
    "* e.g. leaking through normalization is a prominent case\n",
    "\n",
    "<center>\n",
    "<img src=\"figs/leakage.png\" alt=\"drawing\" width=\"600\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Leakage across devices\n",
    "\n",
    "* e.g. in DCASE Task 1, **recording devices** play an important role\n",
    "* **generalization performance to unseen devices** should be measured\n",
    "* make sure to use the provided split, splitting data paying no attention to the devices will give you a very wrong estimation of your actual test performance\n",
    "\n",
    "<center>\n",
    "<img src=\"figs/leakage_devices.png\" alt=\"drawing\" width=\"600\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Audio Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Overview\n",
    "\n",
    "audio is processed (amost exclusively) as **spectrograms**. \n",
    "\n",
    "Common preprocessing steps (see <a href=https://github.com/fschmid56/malach23-pipeline> example pipeline on GitHub </a>) involves:\n",
    "\n",
    "* **Pre-emphasis**: time-domain finite impulse response (FIR) filter to modify average spectral shape\n",
    "* **Short Time Fourier Transform**: convert time domain signal into time-frequency domain\n",
    "* Calculate **magnitude** (absolute value) or **power** (squared values) spectrogram\n",
    "* Apply a **mel filterbank** to the spectrogram: match human perception of frequency\n",
    "* Use logarithm $\\rightarrow$ **log mel spectrogram**: match human perception of loudness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Our Starting Point\n",
    "\n",
    "* **Sound**: variation in air pressure at a point in space, as a function of time\n",
    "* Microphone turns mechanical energy of a soundwave into an electrical signal\n",
    "* Analog signal is converted into digital signal in two steps: **discretization at regular time intervals**, **quantization/rounding** each sample to a fixed set of values\n",
    "\n",
    "\n",
    "<center>\n",
    "<img src=\"figs/digitized_signal.png\" alt=\"drawing\" width=\"300\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## The sampling theorem\n",
    "\n",
    "* **a continuous (analog) signal can be properly sampled, only if it does not contain frequency components above one-half of the sampling rate**\n",
    "* from a **properly sampled** signal, we can reconstruct the original signal\n",
    "* e.g. for a signal sampled at 44100 Hz, the **nyquist frequency** is 22050 Hz \n",
    "* frequencies above $\\frac{1}{2}$ of the sampling rate are aliased into frequencies that can be represented in the digitized signal $\\rightarrow$ **information loss** \n",
    "* use analog low pass filter before sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Digital Filters\n",
    "\n",
    "* digital filters are fundamental for digital audio processing\n",
    "* can be used in time or frequency domain\n",
    "* can be used to separate mixed signals, restore clean from distorted signals\n",
    "* are characterized by their **impulse/frequency reponse**\n",
    "* finite and inifinite impulse response filters (FIR, IIR)\n",
    "* **FIR filters implemented using convolution with impulse response**\n",
    "* see **pre-emphasis filter** in example pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Discrete Fourier Transform\n",
    "\n",
    "* Fourier Analysis for **discrete time, discrete frequency, periodic** signals\n",
    "* we pretend our digitized signal repeats inifintely often in both directions \n",
    "* commonly the Fourier Transform is defined in the complex domain\n",
    "\n",
    "$$X[k]=\\frac{1}{N} \\sum_{n=0}^{N-1}x[n] \\mathrm{e}^{\\dfrac{-2\\pi k i n}{N}}$$\n",
    "\n",
    "* using Euler's formula $e^{ix}=\\cos x + i \\sin x$, we can think of the frequency domain as a set of sine and cosine wave amplitudes\n",
    "\n",
    "$$X[k]=\\frac{1}{N} \\sum_{n=0}^{N-1} \\underbrace{x[n] \\cos(\\frac{-2\\pi k n}{N})}_{\\textrm{real part}} + \\underbrace{i x[n] \\sin(\\frac{-2 \\pi k n}{N})}_{\\textrm{imaginary part}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Discrete Fourier Transform\n",
    "\n",
    "Basis functions for 16-point DFT.\n",
    "\n",
    "<center>\n",
    "<img src=\"figs/basis.png\" alt=\"drawing\" width=\"400\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Discrete Fourier Transform\n",
    "\n",
    "* time domain signal runs from $0$ to $N-1$\n",
    "* translates to $\\frac{N}{2} + 1$ sine (imaginary) and cosine (real) amplitudes\n",
    "* DFT can be computed by matrix multiplication with basis function ($O(N^2)$)\n",
    "* ... or much faster using **FFT** ($O(N log N)$)\n",
    "\n",
    "<center>\n",
    "<img src=\"figs/DFT.png\" alt=\"drawing\" width=\"500\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Magnitude and Phase\n",
    "\n",
    "* Cosine and Sine wave amplitudes can be converted to **polar form**:\n",
    "* **Magnitude**: $Mag X[k] = \\sqrt{(Re X[k])^2 + (Im X[k])^2}$\n",
    "* **Phase**: contains little useful information, human ear is insensitive to the relative phase of component sinusoids\n",
    "* In a magnitude spectrum, the **magnitude of a bin $X[k]$ is the sum of all energy in its frequency band**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Frequency Resolution\n",
    "\n",
    "* N-point FFT (N usually chosen as a power of two, signal is zero padded) results in spectrum with $\\frac{N}{2} + 1$ frequency bins\n",
    "* bins are equally spaced in the range $[0, \\frac{S}{2}]$ Hertz (S is sampling rate)\n",
    "* the frequency resolution (in Hertz) is $r \\approx \\frac{S}{N}$\n",
    "* can **reduce inter-sample spacing in frequency domain by zero padding** (increase resolution in frequency domain)\n",
    "\n",
    "<center>\n",
    "<img src=\"figs/zero_padding.png\" alt=\"drawing\" width=\"600\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Spectral Leakage and Windowing\n",
    "\n",
    "* DFT assumes a **periodic signal** $\\rightarrow$ our time-domain signal is **implictly repeated**\n",
    "\n",
    "<center>\n",
    "<img src=\"figs/signal_repeated.png\" alt=\"drawing\" width=\"800\"/>\n",
    "</center>\n",
    "\n",
    "* a sine NOT matching a basis function will be cut at the border, **explaining this cut requires all the basis functions**\n",
    "\n",
    "\n",
    "<center>\n",
    "<img src=\"figs/spectral_leakage.png\" alt=\"drawing\" width=\"400\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Spectral Leakage and Windowing\n",
    "\n",
    "* signals can be multiplied with a window to **avoid hard cuts at the borders**\n",
    "* results in comparable peaks for sines matching or not matching a basis function\n",
    "* popular windows: Hann, Hamming, Gaussian ... \n",
    "* trade-off between **width of the peak** (resolution) and **spectral leakage** (amplitude of tails)\n",
    "\n",
    "<center>\n",
    "<img src=\"figs/windowing.png\" alt=\"drawing\" width=\"600\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Short-Time Fourier Transform\n",
    "\n",
    "* computing DFT of the whole audio clip, we **loose all timing information**\n",
    "* audio clip is cut up in **overlapping frames** and **DFT is computed per frame**\n",
    "* **window length**: make frames long enough to have a **useful frequency resolution** and short enough such that the **signal is stationary within a frame**\n",
    "\n",
    "<center>\n",
    "<img src=\"figs/stft.png\" alt=\"drawing\" width=\"400\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Mel Spectrogram\n",
    "\n",
    "* we usually **apply additional filterbanks** on the spectrogram computed by STFT\n",
    "* can be viewed as a matrix multiplication: $\\mathrm{torch.matmul(filterbank, spectrogram)}$\n",
    "* want to **decrease dimensionality** and keep relevant audio features for task $\\rightarrow$ idea is to model human perception\n",
    "* humans don't perceive frequencies on a **linear scale**; **detecting differences in lower frequencies is easier than in higher frequencies**\n",
    "* want to have higher resolution in low frequency regions, lower resolution in high frequency regions\n",
    "* **Mel scale**: equal distances on mel scale sound equally distant to listeners"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Mel Spectrogram\n",
    "\n",
    "<center>\n",
    "<img src=\"figs/mel_filterbank.png\" alt=\"drawing\" width=\"600\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def wav_plot(x, sr, listen=True, title=\"\"):\n",
    "    fig, ax = plt.subplots(nrows=1, figsize=(10, 2))\n",
    "    ax.set_title(title)\n",
    "    lirod.waveshow(x, sr=sr, ax=ax, x_axis='s')\n",
    "    plt.show()\n",
    "    if listen:\n",
    "        audio = ipd.Audio(x, rate=sr)\n",
    "        ipd.display(audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def freq_plot(x, sr, title=\"\", n=1024, log_freq=True):\n",
    "    x_mag = np.abs(rfft(x, n=n))\n",
    "    x_mag = liro.amplitude_to_db(x_mag)\n",
    "    freqs = rfftfreq(n, 1 / sr)\n",
    "    fig, ax = plt.subplots(nrows=1, figsize=(10, 2))\n",
    "    ax.set_title(title)\n",
    "    if log_freq:\n",
    "        ax.plot(freqs, x_mag)\n",
    "        ax.set_xscale('symlog', base=2)\n",
    "    else:\n",
    "        ax.plot(x_mag)\n",
    "    ax.set_xlabel(\"Frequency (Hertz)\")\n",
    "    ax.set_ylabel(\"Amplitude (dB)\")\n",
    "    ax.xaxis.set_major_formatter(matplotlib.ticker.ScalarFormatter())\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def spec_liro(x, sr, n_fft=1024, win_length=800, hop_length=320, x_is_spec=False,\n",
    "              x_is_power_spec=False, x_is_mel_spec=False, convert_to_db=True, title=\"Spectrogram\"):\n",
    "    fig, ax = plt.subplots(nrows=1, figsize=(10, 2))\n",
    "    ax.set_title(title)\n",
    "    if x_is_mel_spec:\n",
    "        x_is_power_spec = True\n",
    "        x_is_spec = True\n",
    "    if x_is_power_spec:\n",
    "        x_is_spec = True\n",
    "    if not x_is_spec:\n",
    "        x = liro.stft(x, n_fft=n_fft, win_length=win_length, hop_length=hop_length)\n",
    "        x, phase = liro.magphase(x)\n",
    "    if convert_to_db:\n",
    "        if x_is_power_spec:\n",
    "            # power spectrogram \n",
    "            x = liro.power_to_db(x)\n",
    "        else:\n",
    "            # magnitude spectrogram\n",
    "            x = liro.amplitude_to_db(x) \n",
    "    img = lirod.specshow(\n",
    "        x,\n",
    "        sr=sr,\n",
    "        x_axis='s',\n",
    "        y_axis='mel' if x_is_mel_spec else 'log',\n",
    "        cmap='magma',\n",
    "        ax=ax,\n",
    "        n_fft=n_fft,\n",
    "        win_length=win_length\n",
    "    )\n",
    "    fig.colorbar(img, ax=ax, format='%+2.0f dB' if convert_to_db else None)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## An Example: from the waveform to the log mel spectrogram\n",
    "\n",
    "Our starting point: ... a dog barking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "sr = 32000 \n",
    "example_wav, _ = liro.load(example_file, sr=sr)\n",
    "wav_plot(example_wav, sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Apply Pre-emphasis (Digital Filter)\n",
    "\n",
    "* Majority of energy is concentrated in the lower end of the spectrum (drops around 2 dB/kHz)\n",
    "* Pre-emphasis **compensates for the average spectral shape** and **emphasises higher frequencies**\n",
    "* implemented as **time-domain FIR**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "preemphasis_coefficient = torch.as_tensor([[[-.97, 1]]])\n",
    "wav_torch = torch.from_numpy(example_wav)\n",
    "wav_pree = nn.functional.conv1d(wav_torch.reshape(1, 1, -1), preemphasis_coefficient).squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "freq_plot(preemphasis_coefficient.squeeze().numpy(), sr, title=\"Pre-emphasis Filter Frequency Magnitude Reponse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "spec_liro(example_wav, sr, title=\"Log Spectrogram (without Pre-emphasis)\")\n",
    "spec_liro(wav_pree.squeeze().numpy(), sr, title=\"Log Spectrogram (with Pre-emphasis)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## STFT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "n_fft, win_length, hop_length = 1024, 800, 320 \n",
    "window = torch.hann_window(win_length, periodic=False)\n",
    "spec = torch.stft(wav_pree, n_fft=n_fft, hop_length=hop_length, win_length=win_length,\n",
    "                       center=True, normalized=False, window=window, return_complex=False)\n",
    "power_spec = (spec ** 2).sum(dim=-1)\n",
    "# for comparison, we calculate also the magnitude spectrogram\n",
    "mag_spec = torch.sqrt(power_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "wav_plot(window.squeeze().numpy(), sr, listen=False, title=\"Hann window (time-domain)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "spec_liro(mag_spec.squeeze().numpy(), sr, x_is_spec=True, convert_to_db=False, title=\"Magnitude Spectrogram\")\n",
    "spec_liro(power_spec.squeeze().numpy(), sr, x_is_power_spec=True, convert_to_db=False, title=\"Power Spectrogram\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Mel Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "n_mels, fmin, fmax = 40, 0.0, sr // 2\n",
    "mel_basis, _ = torchaudio.compliance.kaldi.get_mel_banks(n_mels, n_fft, sr,\n",
    "                                                                 fmin, fmax, vtln_low=100.0, vtln_high=-500.,\n",
    "                                                                 vtln_warp_factor=1.0)\n",
    "# pad with one zero per mel bin to match n_fft // 2 + 1\n",
    "mel_basis = torch.as_tensor(torch.nn.functional.pad(mel_basis, (0, 1), mode='constant', value=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, figsize=(10, 4))\n",
    "ax.set_title(\"Mel filterbank\")\n",
    "ax.set_xlabel(\"FFT bin index\")\n",
    "ax.set_ylabel(\"Mel bin\")\n",
    "ax.imshow(mel_basis.squeeze().numpy(), cmap='hot', interpolation='nearest', aspect='auto')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "melspec = torch.matmul(mel_basis, power_spec)\n",
    "spec_liro(power_spec.squeeze().numpy(), sr, x_is_power_spec=True, title=\"Log Power Spectrogram\")\n",
    "spec_liro(melspec.squeeze().numpy(), sr, x_is_mel_spec=True, title=\"Log Mel Spectrogram\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##  Why the 'Log' in amplitude?\n",
    "\n",
    "**Perceived loudness** and **sound power** have an **exponential relationship**. Increasing the sound power by a factor of 10 increases the perceived loudness by about a factor of 2. We account for that by taken the logarithm of sound power in our preprocessing pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "log_mel_spec = (melspec + 0.00001).log()\n",
    "spec_liro(melspec.squeeze().numpy(), sr, x_is_mel_spec=True, convert_to_db=False, title=\"Mel Spectrogram\")\n",
    "spec_liro(log_mel_spec.squeeze().numpy(), sr, x_is_mel_spec=True, convert_to_db=False, title=\"Log Mel Spectrogram\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Normalization\n",
    "\n",
    "We often want to **normalize spectrograms by dataset mean and variance**. Below we use approximated mean (-4.5) and standard deviation (5). However, **running through the dataset** and calculating mean and variance is the usual way to go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "norm_log_mel_spec = (log_mel_spec + 4.5) / 5.\n",
    "spec_liro(log_mel_spec.squeeze().numpy(), sr, x_is_mel_spec=True, convert_to_db=False, title=\"Log Mel Spectrogram\")\n",
    "spec_liro(norm_log_mel_spec.squeeze().numpy(), sr, x_is_mel_spec=True, convert_to_db=False, title=\"Norm. Log Mel Spectrogram\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### What to do with a normalized log mel spectrogram?\n",
    "\n",
    "**Use your favourite vision architecture and treat the normalized log mel spectrogram as an image with a single input channel.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Example Pipeline on GitHub\n",
    "\n",
    "The <a href=https://github.com/fschmid56/malach23-pipeline> example ML4Audio pipeline </a> demonstrates the following points based on 200 wav files:\n",
    "\n",
    "* Dataset loading, PyTorch Dataset class, PyTorch Dataloader\n",
    "* our pre-processing routine that we discussed today\n",
    "* how to use a PyTorch Model (CNN) to generate predictions based on a log mel spectrogram\n",
    "* simple data augmentation techniques (masking time frames, masking frequency bands, mixup, time rolling)\n",
    "* a training loop implemented with PyTorch Lightening\n",
    "* logging implemented with Weights and Biases\n",
    "* **some** of the best practices we discussed today"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Your next task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Get comfortable with the <a href=https://github.com/fschmid56/malach23-pipeline> example ML4Audio pipeline </a> \n",
    "* Re-read the task description and pay special attention to the data split provided\n",
    "* Modify the example pipeline and **set up the DCASE baseline system** for your task\n",
    "* Prepare a short report about the baseline system for your task including logged metrics of a (hopefully) successful experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Questions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Do you feel comfortable with today's content?\n",
    "* Do you have computing resources available? Do you need resources from us?\n",
    "* \\<your questions\\>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Your Reports"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
