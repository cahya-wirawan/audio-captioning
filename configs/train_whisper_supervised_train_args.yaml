do_train: True
do_eval: True

max_steps: 100_000
optim: "adamw_torch"
learning_rate: 0.00005 # TODO
warmup_steps: 500 # TODO

# TODO all of these
per_device_train_batch_size: 1
gradient_accumulation_steps: 16
per_device_eval_batch_size: 1
eval_accumulation_steps: 8

# Don't touch this
logging_steps: 50
report_to: "wandb"

# TODO check if this makes training faster (or use less memory) in our setup
fp16: True

metric_for_best_model: "sacrebleu" # TODO change
greater_is_better: True # TODO change according to metric

load_best_model_at_end: True
predict_with_generate: True # NEVER TOUCH THIS
generation_num_beams: 1 # TODO
generation_max_length: 40
evaluation_strategy: "steps"
eval_steps: 500 # TODO

save_strategy: "steps"
save_steps: 500 # TODO
save_total_limit: 10 # TODO
